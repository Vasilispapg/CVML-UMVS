{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from math import floor,sqrt,ceil,log2\n",
    "from moviepy.editor import VideoFileClip\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import h5py\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import objectDetection as od\n",
    "from tabulate import tabulate\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "import h5py\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import isodata\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "\n",
    "import spacy_sentence_bert\n",
    "tokenizer = spacy_sentence_bert.load_model('en_stsb_roberta_large')\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "import sys\n",
    "sys.path.append('AutoEncoder')\n",
    "from autoEncoder import reduce_features_with_autoencoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Video Processing and Frame Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_rate=15):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    success = True\n",
    "    frames = []\n",
    "    \n",
    "    while success:\n",
    "        success, image = video.read()\n",
    "        if count % frame_rate == 0 and success:\n",
    "            frames.append(image)\n",
    "        count += 1\n",
    "\n",
    "    video.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Audio Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_audio_from_video(video_path, output_audio_path):\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(output_audio_path)\n",
    "    video.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features_for_each_frame(audio_path, frame_rate,num_frames=None):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    \n",
    "    # frame_rate 1/1 = 30\n",
    "    # frame_rate 1/2 = 15\n",
    "\n",
    "    # Calculate the number of audio samples per video frame\n",
    "    samples_per_frame = sr / frame_rate\n",
    "    \n",
    "    print(\"samples_per_frame\",samples_per_frame)\n",
    "    print(\"len(y)\",len(y))\n",
    "    print(\"sr\",sr)\n",
    "    print(\"frame_rate\",frame_rate)\n",
    "\n",
    "    # Initialize an array to store MFCCs for each frame\n",
    "    mfccs_per_frame = []\n",
    "\n",
    "    # Iterate over each frame and extract corresponding MFCCs\n",
    "    for frame in range(int(len(y) / samples_per_frame)):\n",
    "        start_sample = int(frame * samples_per_frame)\n",
    "        end_sample = int((frame + 1) * samples_per_frame)\n",
    "\n",
    "        # Ensure the end sample does not exceed the audio length\n",
    "        end_sample = min(end_sample, len(y))\n",
    "\n",
    "        # Extract MFCCs for the current frame's audio segment\n",
    "        mfccs_current_frame = librosa.feature.mfcc(y=y[start_sample:end_sample], sr=sr, n_mfcc=130)\n",
    "        mfccs_processed = np.mean(mfccs_current_frame.T, axis=0)\n",
    "        mfccs_per_frame.append(mfccs_processed)\n",
    "\n",
    "    if(len(mfccs_per_frame)>num_frames):\n",
    "        return mfccs_per_frame[:num_frames]\n",
    "    return mfccs_per_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Visual Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_color_features(frame, bins=512):\n",
    "    \"\"\"\n",
    "    Extract color histogram features from a frame.\n",
    "\n",
    "    :param frame: The frame from the video (as a NumPy array).\n",
    "    :param bins: Number of bins for the histogram.\n",
    "    :return: Normalized color histogram feature.\n",
    "    \"\"\"\n",
    "    # Calculate the histogram for each color channel\n",
    "    hist_features = []\n",
    "    for i in range(3):  # Assuming frame is in BGR format\n",
    "        hist = cv2.calcHist([frame], [i], None, [bins], [0, 256])\n",
    "        hist = cv2.normalize(hist, hist).flatten()\n",
    "        hist_features.extend(hist)\n",
    "    \n",
    "    return np.array(hist_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optical_flow(prev_frame, curr_frame):\n",
    "    \"\"\"\n",
    "    Compute optical flow between two frames.\n",
    "\n",
    "    :param prev_frame: The previous frame in the video.\n",
    "    :param curr_frame: The current frame in the video.\n",
    "    :return: Optical flow magnitude and angle.\n",
    "    \"\"\"\n",
    "    # Convert frames to grayscale\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate optical flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    # Compute magnitude and angle of the flow vectors\n",
    "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    \n",
    "    # Normalize and flatten\n",
    "    magnitude = cv2.normalize(magnitude, None, 0, 1, cv2.NORM_MINMAX).flatten()\n",
    "    angle = angle.flatten()\n",
    "\n",
    "    return magnitude, angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Load the weights from the downloaded file\n",
    "base_model = VGG16(weights=None, include_top=False)\n",
    "weights_path = 'vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5' # Replace with the actual path\n",
    "base_model.load_weights(weights_path)\n",
    "\n",
    "# Create a new Sequential model and add the VGG16 base model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "\n",
    "def extract_visual_features(frames):\n",
    "    features = []\n",
    "    for frame in frames:\n",
    "        if frame is not None:\n",
    "            img = cv2.resize(frame, (224, 224))  # Resize frame to 224x224\n",
    "            img = img_to_array(img)              # Convert to array\n",
    "            img = np.expand_dims(img, axis=0)    # Add batch dimension\n",
    "            img = preprocess_input(img)          # Preprocess for VGG16\n",
    "            \n",
    "            feature = model.predict(img,use_multiprocessing=True,workers=4)\n",
    "            features.append(feature.flatten())\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def integrate_features(frames,vgg_features=None,encoding_dim=256, autoencoder_epochs=50, autoencoder_batch_size=32):\n",
    "    \"\"\"\n",
    "    Integrate color, motion, and VGG features for a list of frames.\n",
    "\n",
    "    :param frames: List of frames from the video.\n",
    "    :param bins: Number of bins for color histogram.\n",
    "    :return: Integrated feature vector for each frame.\n",
    "    \"\"\"\n",
    "    integrated_features = []\n",
    "    if(vgg_features is None):\n",
    "        vgg_features = extract_visual_features(frames) \n",
    "        \n",
    "    for i in range(1, len(frames)):\n",
    "        color_features = extract_color_features(frames[i])\n",
    "        magnitude, angle = compute_optical_flow(frames[i-1], frames[i])\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate([color_features, magnitude, angle, vgg_features[i]])\n",
    "        integrated_features.append(combined_features)\n",
    "    # Reduce features using autoencoder\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='mean')  # Can also use 'median' or 'most_frequent'\n",
    "    integrated_features = imputer.fit_transform(integrated_features)\n",
    "\n",
    "    # integrated_features = StandardScaler().fit_transform(integrated_features)\n",
    "    \n",
    "    reduced_features = reduce_features_with_autoencoder(np.array(integrated_features), encoding_dim, autoencoder_epochs, autoencoder_batch_size)\n",
    "    \n",
    "    return reduced_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect Audio/Annotation/Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title Tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfTitle(info_file,video_path):\n",
    "    info_df = pd.read_csv(info_file, sep='\\t')\n",
    "    info_video=info_df[info_df['video_id']==video_path.split('/')[-1].split('.')[0]]\n",
    "    title=info_video['title'].values[0]\n",
    "    # Preprocess titles and extract features (TF-IDF or word embeddings)\n",
    "    # For TF-IDF:\n",
    "    print('title:',title)\n",
    "\n",
    "    title_features = tokenizer(title).vector\n",
    "    print('title_features:',title_features.shape)\n",
    "    return np.array(title_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(video_path, anno_file, info_file,flag_to_extract):\n",
    "    \n",
    "    return_data=[]\n",
    "    # Extract frames from the video\n",
    "    if(flag_to_extract[0]):\n",
    "        frames = extract_frames(video_path)\n",
    "        return_data.append(['frames',frames])\n",
    "    else:\n",
    "        return_data.append(None)\n",
    "\n",
    "    # # Extract visual features\n",
    "    if(flag_to_extract[1]):\n",
    "        visual_features = extract_visual_features(frames) \n",
    "        return_data.append(['visual',visual_features])\n",
    "    else:\n",
    "        return_data.append(None)\n",
    "\n",
    "    # Extract audio\n",
    "    if(flag_to_extract[2]):\n",
    "        audio_output_path = 'datasets/extractedAudio/extracted_audio.wav'\n",
    "        extract_audio_from_video(video_path, audio_output_path) \n",
    "\n",
    "        # Extract audio features\n",
    "        audio_features = extract_audio_features_for_each_frame(audio_output_path,30,len(frames))\n",
    "        return_data.append(['audio',audio_features])\n",
    "    else:\n",
    "        return_data.append(None)\n",
    "\n",
    "    # Load titles from info file\n",
    "    if(flag_to_extract[3]):\n",
    "        title_features = tfTitle(info_file,video_path)\n",
    "        return_data.append(['title',title_features])\n",
    "    else:\n",
    "        return_data.append(None)\n",
    "\n",
    "\n",
    "    return return_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detect objects and encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectObjects(frames, yolo_model, classes, objects=None,encoded_objects=None,video=None):\n",
    "    if objects is None:\n",
    "        print('Detecting objects in frames...')\n",
    "        yolo_model, classes = loadYOLOv5()\n",
    "        \n",
    "        objects = od.detect_objects_in_all_frames(frames, yolo_model, classes)\n",
    "        saveData('objects',objects,video)\n",
    "\n",
    "    # Here, taking the first detected object\n",
    "    objects = [frame_objects if frame_objects else ['None'] for frame_objects in objects]\n",
    "\n",
    "    count=0\n",
    "    for obj in objects:\n",
    "        for o in obj:\n",
    "            count+=1\n",
    "            \n",
    "    print(f'Total Encoded Obj: {count}')\n",
    "    print(\"Objects:\",len(objects))\n",
    "\n",
    "    # One-hot encoding of objects\n",
    "    if encoded_objects is None:\n",
    "        encoded_objects = []\n",
    "        for frame_objects in objects:\n",
    "            # Encode each object in the frame\n",
    "            encoded_frame_objects = [tokenizer(ob).vector for ob in frame_objects]\n",
    "            # Add the list of encoded objects for this frame to the main list\n",
    "            encoded_objects.append(encoded_frame_objects)\n",
    "        \n",
    "    \n",
    "    return encoded_objects, objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Data if ness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def saveData(name,feature,video):\n",
    "    if not os.path.exists(f'video_ext_data/{video}'):\n",
    "        os.makedirs(f'video_ext_data/{video}')\n",
    "    with open(f'video_ext_data/{video}/{name}.pkl', 'wb') as f:\n",
    "        pickle.dump(feature,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data if ness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def getData(feature,video):\n",
    "    if os.path.exists(f'video_ext_data/{video}/{feature}.pkl'):\n",
    "        with open(f'video_ext_data/{video}/{feature}.pkl', 'rb') as f:\n",
    "            feature = pickle.load(f)\n",
    "        return feature\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_frames_with_title_object(integrated_features, title_vector, object_vectors, bins=32):\n",
    "\n",
    "    frame_scores = []\n",
    "\n",
    "    for i, frame_features in enumerate(integrated_features):\n",
    "        # Assuming object_vectors[i] is the vector for objects in the ith frame\n",
    "        # Adjust the logic here if you have a different way of storing object vectors\n",
    "\n",
    "        # Calculate similarity or distance\n",
    "        title_similarity = cosine_distance(frame_features, title_vector)\n",
    "        sum_obj_sim=0\n",
    "        for obj in object_vectors[i]:\n",
    "            \n",
    "            object_similarity = cosine_distance(frame_features, obj)\n",
    "            sum_obj_sim+=object_similarity\n",
    "\n",
    "        # Combine these similarities into a single score\n",
    "        # This can be a simple average, weighted sum, or any other method that makes sense for your application\n",
    "        combined_score = (title_similarity + sum_obj_sim) / 2\n",
    "\n",
    "        frame_scores.append(combined_score)\n",
    "\n",
    "    return frame_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataExtraction(video_path, anno_file, info_file,getDataFlag=False):\n",
    "    \"\"\"\n",
    "    Integrate visual, audio, and annotation features from a video,\n",
    "    and perform clustering on the combined features.\n",
    "\n",
    "    :param video_path: Path to the video file.\n",
    "    :param anno_file: Path to the annotation file.\n",
    "    :param info_file: Path to the info file.\n",
    "    :param num_clusters: Number of clusters to use in KMeans.\n",
    "    :return: Cluster labels for each data point.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract data from video\n",
    "    objects=None\n",
    "    \n",
    "    video=video_path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    # GetData\n",
    "    objects=getData('objects',video)\n",
    "    frames=getData('frames',video)\n",
    "    visual_features=getData('visual',video)\n",
    "    audio_features=getData('audio',video)\n",
    "    title_features=getData('title',video)\n",
    "    encoded_objects=getData('encoded_objects',video)\n",
    "    \n",
    "    flag_to_extract=[True,True,True,True]\n",
    "    \n",
    "    if(frames is not None):\n",
    "        flag_to_extract[0]=False\n",
    "    if(visual_features is not None):\n",
    "        flag_to_extract[1]=False\n",
    "    if(audio_features is not None):\n",
    "        flag_to_extract[2]=False\n",
    "    if(title_features is not None):\n",
    "        flag_to_extract[3]=False\n",
    "\n",
    "    \n",
    "    if not getDataFlag:\n",
    "        # Extract data from video and save it\n",
    "        data=extractData(video_path, anno_file, info_file,flag_to_extract)\n",
    "        # Save extracted Data\n",
    "        for d in data:\n",
    "            if d is not None:\n",
    "                if(d[0]=='objects'):\n",
    "                    objects=d[1]\n",
    "                elif(d[0]=='frames'):\n",
    "                    frames=d[1]\n",
    "                elif(d[0]=='visual'):\n",
    "                    visual_features=d[1]\n",
    "                elif(d[0]=='audio'):\n",
    "                    audio_features=d[1]\n",
    "                elif(d[0]=='title'):\n",
    "                    title_features=d[1]\n",
    "                \n",
    "                saveData(d[0],d[1],video)\n",
    "                \n",
    "    if(objects is None):\n",
    "        encoded_objects,objects = detectObjects(frames,len(visual_features),len(audio_features),encoded_objects=encoded_objects,video=video)\n",
    "    else:\n",
    "        encoded_objects,objects = detectObjects(frames,len(visual_features),len(audio_features),objects,encoded_objects=encoded_objects)\n",
    "        \n",
    "    saveData('encoded_objects',encoded_objects,video)\n",
    "    \n",
    "\n",
    "    integrated_features = integrate_features(frames,visual_features)\n",
    "    score=score_frames_with_title_object(integrated_features, title_features, encoded_objects, bins=32)\n",
    "    print(\"SCORE:\",score)\n",
    "    \n",
    "    \n",
    "    return [encoded_objects,title_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_from_frames(frames, output_path, frame_rate=30):\n",
    "    if not frames:\n",
    "        print(\"No frames to create a video.\")\n",
    "        return None\n",
    "    # Determine the width and height from the first frame\n",
    "    height, width, layers = frames[0].shape\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, frame_rate, (width, height))\n",
    "    \n",
    "    # Write each frame to the video\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release the VideoWriter object\n",
    "    out.release()\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load .Mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_titles(encoded_titles, hdf5_file):\n",
    "    decoded_titles = []\n",
    "    for ref_array in encoded_titles:\n",
    "        # Handle the case where each ref_array might contain multiple references\n",
    "        for ref in ref_array:\n",
    "            # Dereference each HDF5 object reference to get the actual data\n",
    "            title_data = hdf5_file[ref]\n",
    "            # Decode the title\n",
    "            decoded_title = ''.join(chr(char[0]) for char in title_data)\n",
    "            decoded_titles.append(decoded_title)\n",
    "    return decoded_titles\n",
    "\n",
    "\n",
    "def load_mat_file(file_path,videoID):\n",
    "    \"\"\"\n",
    "    Load a .mat file and return its contents.\n",
    "\n",
    "    :param file_path: Path to the .mat file.\n",
    "    :return: Contents of the .mat file.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        user_anno_refs=file['tvsum50']['user_anno'][:] # type: ignore\n",
    "        video_refs=file['tvsum50']['video'][:] # type: ignore\n",
    "\n",
    "        decoded_videos = decode_titles(video_refs,file)\n",
    "    \n",
    "        annotations = []        \n",
    "        # Get the index from decoded video list to find the annotation for the video\n",
    "        index = [i for i, x in enumerate(decoded_videos) if x.lower() in videoID.lower()][0]\n",
    "        \n",
    "        # Iterate over each reference\n",
    "        for ref in user_anno_refs:\n",
    "            # Dereference each HDF5 object reference\n",
    "            ref_data = file[ref[0]]\n",
    "\n",
    "            # Convert to NumPy array and add to the annotations list\n",
    "            annotations.append(np.array(ref_data))\n",
    "            \n",
    "        return annotations[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summary(predicted_summary, user_summary, eval_method='avg'):\n",
    "    max_len = max(len(predicted_summary), user_summary.shape[1])\n",
    "    S = np.zeros(max_len, dtype=int)\n",
    "    G = np.zeros(max_len, dtype=int)\n",
    "    S[:len(predicted_summary)] = predicted_summary\n",
    "\n",
    "    f_scores = []\n",
    "    for user in range(user_summary.shape[0]):\n",
    "        G[:user_summary.shape[1]] = user_summary[user]\n",
    "        overlapped = S & G\n",
    "        \n",
    "        precision = sum(overlapped) / sum(S) if sum(S) != 0 else 0\n",
    "        recall = sum(overlapped) / sum(G) if sum(G) != 0 else 0\n",
    "        f_score = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        f_scores.append(f_score * 100)  # multiplied by 100 for percentage\n",
    "\n",
    "    if eval_method == 'max':\n",
    "        return max(f_scores)\n",
    "    else:  # 'avg'\n",
    "        return sum(f_scores) / len(f_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_frame_selection(ground_truth, summary_indices):\n",
    "    \"\"\"\n",
    "    Evaluate the selected frames by comparing them with the ground truth.\n",
    "    \n",
    "    Args:\n",
    "    ground_truth: Ground truth annotations.\n",
    "    summary_indices: Indices of the selected frames.\n",
    "    \n",
    "    Returns:\n",
    "    Average importance score, max importance score, and proportion of frames with high importance score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluate the selected frames\n",
    "    selected_importance_scores = ground_truth[summary_indices]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if selected_importance_scores.size == 0:\n",
    "        average_importance = 0\n",
    "        max_importance = 0\n",
    "        proportion_high_importance = 0\n",
    "    else:\n",
    "        average_importance = np.mean(selected_importance_scores)  # Average importance score\n",
    "        max_importance = np.max(selected_importance_scores)  # Max importance score\n",
    "        # Calculate the proportion of frames with high importance score\n",
    "        proportion_high_importance = np.mean(selected_importance_scores >= np.floor(max_importance))\n",
    "        \n",
    "    return average_importance, max_importance,proportion_high_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation(ground_truth_path,summary_indices,videoID):\n",
    "    \n",
    "    # Get the ground_truth\n",
    "    ground_truth = np.array(load_mat_file(ground_truth_path, videoID))\n",
    "\n",
    "    f_score_max = evaluate_summary(summary_indices, ground_truth, 'max')\n",
    "    f_score_avg = evaluate_summary(summary_indices, ground_truth)\n",
    "\n",
    "    \n",
    "    print(f'F-scoreA: {f_score_avg:.2}%')\n",
    "    print(f'F-scoreM: {f_score_max:.2}%')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KnapSack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knapsack_for_video_summary(values, weights, capacity, scale_factor=5):\n",
    "    \"\"\"\n",
    "    Apply the 0/1 Knapsack algorithm to select video segments for summarization.\n",
    "\n",
    "    :param values: List of importance scores for each segment.\n",
    "    :param weights: List of durations for each segment in seconds.\n",
    "    :param capacity: Maximum total duration for the summary in seconds.\n",
    "    :param scale_factor: Factor to scale weights to integers.\n",
    "    :return: Indices of the segments to include in the summary.\n",
    "    \"\"\"\n",
    "    # Scale weights and capacity\n",
    "    weights = [int(w * scale_factor) for w in weights]\n",
    "    capacity = int(capacity * scale_factor)\n",
    "\n",
    "    n = len(values)\n",
    "    K = [[0 for _ in range(capacity + 1)] for _ in range(n + 1)]\n",
    "\n",
    "    # Build table K[][] in a bottom-up manner\n",
    "    for i in range(n + 1):\n",
    "        for w in range(capacity + 1):\n",
    "            if i == 0 or w == 0:\n",
    "                K[i][w] = 0\n",
    "            elif weights[i-1] <= w:\n",
    "                K[i][w] = max(values[i-1] + K[i-1][w-weights[i-1]], K[i-1][w])\n",
    "            else:\n",
    "                K[i][w] = K[i-1][w]\n",
    "\n",
    "    # Find the selected segments\n",
    "    res = K[n][capacity]\n",
    "    w = capacity\n",
    "    selected_indices = []\n",
    "\n",
    "    for i in range(n, 0, -1):\n",
    "        if res <= 0:\n",
    "            break\n",
    "        if res == K[i-1][w]:\n",
    "            continue\n",
    "        else:\n",
    "            selected_indices.append(i-1)\n",
    "            res = res - values[i-1]\n",
    "            w = w - weights[i-1]\n",
    "\n",
    "    selected_indices.reverse()\n",
    "    return selected_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### importance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vec1, vec2):\n",
    "    return np.linalg.norm(np.array(vec1) - np.array(vec2))\n",
    "\n",
    "def manhattan_distance(vec1, vec2):\n",
    "    return np.sum(np.abs(np.array(vec1) - np.array(vec2)))\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_distance(vec1, vec2):\n",
    "    # Ensure the vectors are 2D and of the same length\n",
    "    vec1 = vec1.reshape(1, -1)\n",
    "    vec2 = vec2.reshape(1, -1)\n",
    "\n",
    "    # Check if either vector contains only NaNs\n",
    "    if np.isnan(vec1).all() or np.isnan(vec2).all():\n",
    "        return 0  # or handle as needed\n",
    "\n",
    "    return cosine_similarity(vec1, vec2)[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_scores(scores):\n",
    "    min_score = np.min(scores)\n",
    "    max_score = np.max(scores)\n",
    "\n",
    "    # Avoid division by zero in case all scores are the same\n",
    "    if max_score == min_score:\n",
    "        return np.zeros_like(scores)\n",
    "\n",
    "    normalized_scores = (scores - min_score) / (max_score - min_score)\n",
    "    return normalized_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_importance_ManDistFrame(title, objects):\n",
    "    importance_scores = []\n",
    "\n",
    "    for frame_objects in objects:\n",
    "        frame_distance = 0\n",
    "\n",
    "        for obj in frame_objects:\n",
    "            obj_array = np.array(obj)\n",
    "            title_array = np.array(title)\n",
    "\n",
    "            # Calculate Manhattan distance and sum it up for the frame\n",
    "            distance = euclidean_distance(obj_array, title_array)\n",
    "            frame_distance += distance\n",
    "\n",
    "        # Store the summed distance for the frame\n",
    "        importance_scores.append(frame_distance)\n",
    "\n",
    "    importance_scores=normalize_scores(importance_scores)\n",
    "    \n",
    "    # Invert the scores because lower Manhattan distance indicates higher similarity\n",
    "    importance_scores = 1 - importance_scores\n",
    "    \n",
    "    # Handle zero scores to avoid division by zero later\n",
    "    for i in range(len(importance_scores)):\n",
    "        if importance_scores[i] == 0:\n",
    "            importance_scores[i] = 0.0001\n",
    "            \n",
    "    return importance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map labels with frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_frames_to_labels_with_indices(frames, labels):\n",
    "    label_frame_dict = {}\n",
    "    for label, (frame_index, frame) in zip(labels, enumerate(frames)):\n",
    "        # Convert label from numpy array to scalar if necessary\n",
    "        label_scalar = label.item() if isinstance(label, np.ndarray) else label\n",
    "        if label_scalar not in label_frame_dict:\n",
    "            label_frame_dict[label_scalar] = []\n",
    "        label_frame_dict[label_scalar].append((frame_index, frame))\n",
    "    return label_frame_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete pkl Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deletePKLfiles(video):\n",
    "    dirpkl='video_ext_data/'+video+'/'\n",
    "    if(os.path.exists(dirpkl+'frames.pkl')):\n",
    "        os.remove(dirpkl+\"frames.pkl\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to create summary video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path='datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-anno.tsv'\n",
    "info_path='datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-info.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path='datasets/ydata-tvsum50-v1_1/video/'\n",
    "summary_video_path='datasets/summary_videos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path='datasets/ydata-tvsum50-v1_1/ground_truth/ydata-tvsum50.mat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the list of the videos in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list = [video for video in os.listdir(video_path) if video.endswith('.mp4')]  # List comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Yolo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def loadYOLOv5():\n",
    "    # Load the model\n",
    "    yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True,)\n",
    "\n",
    "    with open(\"yolo/coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    return yolo_model,classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map the mat file with h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last change point and total frames for each video in the .h5 file\n",
    "def get_video_data_from_h5(file_path):\n",
    "    video_data_h5 = []\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        for video_id in file.keys():\n",
    "            last_change_point = file[str(video_id)]['change_points'][-1]\n",
    "            total_frames = last_change_point[1]\n",
    "            video_data_h5.append([video_id, total_frames])\n",
    "    return video_data_h5\n",
    "\n",
    "# Get frame numbers from the .mat file\n",
    "def get_frame_numbers(encoded_frames, hdf5_file):\n",
    "    frame_numbers = []\n",
    "    for ref_array in encoded_frames:\n",
    "        for ref in ref_array:\n",
    "            frame_data = hdf5_file[ref]\n",
    "            frame_numbers.extend([int(char[0]) for char in frame_data])\n",
    "    return frame_numbers\n",
    "\n",
    "# Extract data from .mat file\n",
    "def get_video_data_from_mat(file_path):\n",
    "    video_data_mat = []\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        encoded_videos = f['tvsum50']['video'][:]\n",
    "        encoded_frame_counts = f['tvsum50']['nframes'][:]\n",
    "        decoded_videos = decode_titles(encoded_videos, f)\n",
    "        decoded_frame_counts = get_frame_numbers(encoded_frame_counts, f)\n",
    "        for i, video_id in enumerate(decoded_videos):\n",
    "            video_data_mat.append([video_id, decoded_frame_counts[i]])\n",
    "    return video_data_mat\n",
    "\n",
    "# Comparing and mapping the data\n",
    "h5_file_path = 'datasets/ydata-tvsum50-v1_1/eccv16_dataset_tvsum_google_pool5.h5'\n",
    "mat_file_path = 'datasets/ydata-tvsum50-v1_1/ground_truth/ydata-tvsum50.mat'\n",
    "\n",
    "video_data_h5 = get_video_data_from_h5(h5_file_path)\n",
    "video_data_mat = get_video_data_from_mat(mat_file_path)\n",
    "\n",
    "video_id_map = {}\n",
    "for video_mat in video_data_mat:\n",
    "    for video_h5 in video_data_h5:\n",
    "        if video_mat[1] == video_h5[1] + 1:\n",
    "            video_id_map[video_mat[0]] = video_h5[0]\n",
    "\n",
    "\n",
    "def getChangingPoints(video_id):\n",
    "    with h5py.File(h5_file_path, 'r') as file:\n",
    "        return file[video_id]['change_points'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map to 1/1 frame rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_scores_to_original_frames(sampled_scores, frame_rate):\n",
    "    # Create an empty list to hold the mapped scores\n",
    "    original_scores = []\n",
    "\n",
    "    # Iterate over the sampled scores\n",
    "    for score in sampled_scores:\n",
    "        # Replicate each score frame_rate times\n",
    "        original_scores.extend([score] * frame_rate)\n",
    "\n",
    "    return original_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the importances for each clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClipImportances(importance, video):\n",
    "    \n",
    "    # Get the changing points for the video\n",
    "    changingPoints = getChangingPoints(video_id_map[video.split('.')[0]])\n",
    "        \n",
    "    # Initialize a dictionary to store clip importances\n",
    "    clip_importances = {}\n",
    "\n",
    "    # Iterate over each clip defined by changing points\n",
    "    for clip_index, (start_frame, end_frame) in enumerate(changingPoints):\n",
    "        clip_importance = 0\n",
    "        num_frames=end_frame-start_frame\n",
    "\n",
    "        # Calculate the total importance for this clip\n",
    "        for i in range(start_frame, end_frame):\n",
    "            clip_importance += importance[i]\n",
    "\n",
    "        # Store the clip importance\n",
    "        clip_importances[clip_index] = (clip_importance,num_frames)\n",
    "        \n",
    "    # normalize\n",
    "    return clip_importances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for videoSummarizion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSelectedIndicesFromClips(selectedClips,video):\n",
    "    changingPoints = getChangingPoints(video_id_map[video.split('.')[0]])\n",
    "\n",
    "    # Initialize a list to store the selected indices\n",
    "    selected_indices = []\n",
    "    for i in selectedClips:\n",
    "        selected_indices.extend(range(changingPoints[i][0], changingPoints[i][1]))\n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoSumm(annotation_path=None, info_path=None, video_path=None, summary_video_path=None,video_list=None):\n",
    "    for video in video_list: \n",
    "        video='0tmA_C6XwfM.mp4'\n",
    "        \n",
    "        # if(os.path.exists(f'{summary_video_path}{video}')):\n",
    "        #     continue\n",
    "    \n",
    "        print(\"VIDEO:\",video)\n",
    "        getDataFlag=False\n",
    "\n",
    "        # Extract frames 1/1 from the video\n",
    "        original_frames=extract_frames(video_path+video, frame_rate=1)\n",
    "             \n",
    "        # Extract Data\n",
    "        objects,title_features = DataExtraction(video_path+video, annotation_path, info_path,getDataFlag=getDataFlag)\n",
    "        \n",
    "        # Calculate importance scores for this cluster\n",
    "        importance = calculate_importance_ManDistFrame(title_features, objects)\n",
    "        \n",
    "        # Maping to 1/1 rate\n",
    "        importance=map_scores_to_original_frames(importance, 15)\n",
    "\n",
    "        # get the best cluster\n",
    "        clip_info = getClipImportances(importance,video)\n",
    "        # Extracting values (importance scores) and weights (number of frames)\n",
    "        values = [score for score, frames in clip_info.values()]\n",
    "        weights = [frames for score, frames in clip_info.values()]\n",
    "\n",
    "        # Calculate the total number of frames in the video\n",
    "        total_frames = len(original_frames)\n",
    "        print(\"Total Frames:\",total_frames)\n",
    "\n",
    "        # Calculate the capacity as 15% of the total number of frames\n",
    "        capacity = int(0.16 * total_frames)\n",
    "        print(\"Capacity:\",capacity)\n",
    "\n",
    "        # Now apply the knapsack algorithm\n",
    "        selected_clips = knapsack_for_video_summary(values, weights, capacity)\n",
    "        print(\"Summary Indices:\",selected_clips)\n",
    "        \n",
    "        selected_indices=getSelectedIndicesFromClips(selected_clips,video)\n",
    "        print('Sum Len Frame:',len(selected_indices))\n",
    "        \n",
    "        summary_frames=[original_frames[i] for i in selected_indices]\n",
    "        \n",
    "        # Evaluate\n",
    "        Evaluation(ground_truth_path, selected_indices, video.split('.')[0])\n",
    "        \n",
    "        # print(tabulate([evaluated_metrics], headers=['Threshold', 'Precision', 'Recall', 'F1', 'Avg. Importance', 'Max. Importance', 'Prop. High Importance']))\n",
    "        getDataFlag=True\n",
    "            \n",
    "        video_name=video.split('.')[0]\n",
    "        \n",
    "        if(not os.path.exists(f'video_ext_data/{video_name}')):\n",
    "            os.makedirs(f'video_ext_data/{video_name}')\n",
    "        \n",
    "        # Create Summary Video\n",
    "        create_video_from_frames(summary_frames,f\"{summary_video_path}{video}\" , 30)\n",
    "        \n",
    "        # Extract data from video the next video\n",
    "        getDataFlag=False\n",
    "        deletePKLfiles(video_name)\n",
    "        break\n",
    "    # return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIDEO: 0tmA_C6XwfM.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Encoded Obj: 238\n",
      "Objects: 236\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 22s 3s/step - loss: 7.7892\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 22s 3s/step - loss: 7.1761\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 22s 3s/step - loss: 7.1711\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 22s 3s/step - loss: 7.1707\n",
      "Epoch 5/50\n",
      "1/8 [==>...........................] - ETA: 20s - loss: 7.0024"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvideoSumm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary_video_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 15\u001b[0m, in \u001b[0;36mvideoSumm\u001b[1;34m(annotation_path, info_path, video_path, summary_video_path, video_list)\u001b[0m\n\u001b[0;32m     12\u001b[0m original_frames\u001b[38;5;241m=\u001b[39mextract_frames(video_path\u001b[38;5;241m+\u001b[39mvideo, frame_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Extract Data\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m objects,title_features \u001b[38;5;241m=\u001b[39m \u001b[43mDataExtraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgetDataFlag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgetDataFlag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Calculate importance scores for this cluster\u001b[39;00m\n\u001b[0;32m     18\u001b[0m importance \u001b[38;5;241m=\u001b[39m calculate_importance_ManDistFrame(title_features, objects)\n",
      "Cell \u001b[1;32mIn[15], line 65\u001b[0m, in \u001b[0;36mDataExtraction\u001b[1;34m(video_path, anno_file, info_file, getDataFlag)\u001b[0m\n\u001b[0;32m     60\u001b[0m     encoded_objects,objects \u001b[38;5;241m=\u001b[39m detectObjects(frames,\u001b[38;5;28mlen\u001b[39m(visual_features),\u001b[38;5;28mlen\u001b[39m(audio_features),objects,encoded_objects\u001b[38;5;241m=\u001b[39mencoded_objects)\n\u001b[0;32m     62\u001b[0m saveData(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoded_objects\u001b[39m\u001b[38;5;124m'\u001b[39m,encoded_objects,video)\n\u001b[1;32m---> 65\u001b[0m integrated_features \u001b[38;5;241m=\u001b[39m \u001b[43mintegrate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvisual_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m score\u001b[38;5;241m=\u001b[39mscore_frames_with_title_object(integrated_features, title_features, encoded_objects, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCORE:\u001b[39m\u001b[38;5;124m\"\u001b[39m,score)\n",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m, in \u001b[0;36mintegrate_features\u001b[1;34m(frames, vgg_features, encoding_dim, autoencoder_epochs, autoencoder_batch_size)\u001b[0m\n\u001b[0;32m     23\u001b[0m integrated_features \u001b[38;5;241m=\u001b[39m imputer\u001b[38;5;241m.\u001b[39mfit_transform(integrated_features)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# integrated_features = StandardScaler().fit_transform(integrated_features)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m reduced_features \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_features_with_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintegrated_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoencoder_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoencoder_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduced_features\n",
      "File \u001b[1;32md:\\AUTH\\computer vision\\UnsupervisedVideoSummarization\\AutoEncoder\\autoEncoder.py:50\u001b[0m, in \u001b[0;36mreduce_features_with_autoencoder\u001b[1;34m(features, encoding_dim, epochs, batch_size)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    The `reduce_features_with_autoencoder` function takes in a set of features, creates and trains an\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    autoencoder model to reduce the dimensionality of the features, and returns the reduced features.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    encoding the input features using an autoencoder.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Create and train autoencoder\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_and_train_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Reduce dimensionality\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     reduced_features \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mpredict(features)\n",
      "File \u001b[1;32md:\\AUTH\\computer vision\\UnsupervisedVideoSummarization\\AutoEncoder\\autoEncoder.py:28\u001b[0m, in \u001b[0;36mcreate_and_train_autoencoder\u001b[1;34m(features, encoding_dim, epochs, batch_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)  \u001b[38;5;66;03m# Example of using a lower learning rate\u001b[39;00m\n\u001b[0;32m     27\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoder\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "videoSumm(annotation_path, info_path, video_path, summary_video_path, video_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
